{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9452977",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0031307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/harsh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/harsh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "import nltk.tokenize as token\n",
    "from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "import string\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stop_words.remove(\"very\")\n",
    "# stop_words.add(\"th\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33071dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('gutenberg')    for first time installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "858950ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "files = gutenberg.fileids()\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01e1d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8962e896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n",
      "Number of characters: 887071\n",
      "Number of words: 192427\n",
      "Number of sentences: 7752\n",
      "Number of vocab: 7344\n",
      "5 25 26\n",
      "austen-persuasion.txt\n",
      "Number of characters: 466292\n",
      "Number of words: 98171\n",
      "Number of sentences: 3747\n",
      "Number of vocab: 5835\n",
      "5 26 17\n",
      "austen-sense.txt\n",
      "Number of characters: 673022\n",
      "Number of words: 141576\n",
      "Number of sentences: 4999\n",
      "Number of vocab: 6403\n",
      "5 28 22\n",
      "bible-kjv.txt\n",
      "Number of characters: 4332554\n",
      "Number of words: 1010654\n",
      "Number of sentences: 30103\n",
      "Number of vocab: 12767\n",
      "4 34 79\n",
      "blake-poems.txt\n",
      "Number of characters: 38153\n",
      "Number of words: 8354\n",
      "Number of sentences: 438\n",
      "Number of vocab: 1535\n",
      "5 19 5\n",
      "bryant-stories.txt\n",
      "Number of characters: 249439\n",
      "Number of words: 55563\n",
      "Number of sentences: 2863\n",
      "Number of vocab: 3940\n",
      "4 19 14\n",
      "burgess-busterbrown.txt\n",
      "Number of characters: 84663\n",
      "Number of words: 18963\n",
      "Number of sentences: 1054\n",
      "Number of vocab: 1559\n",
      "4 18 12\n",
      "carroll-alice.txt\n",
      "Number of characters: 144395\n",
      "Number of words: 34110\n",
      "Number of sentences: 1703\n",
      "Number of vocab: 2636\n",
      "4 20 13\n",
      "chesterton-ball.txt\n",
      "Number of characters: 457450\n",
      "Number of words: 96996\n",
      "Number of sentences: 4779\n",
      "Number of vocab: 8335\n",
      "5 20 12\n",
      "chesterton-brown.txt\n",
      "Number of characters: 406629\n",
      "Number of words: 86063\n",
      "Number of sentences: 3806\n",
      "Number of vocab: 7794\n",
      "5 23 11\n",
      "chesterton-thursday.txt\n",
      "Number of characters: 320525\n",
      "Number of words: 69213\n",
      "Number of sentences: 3742\n",
      "Number of vocab: 6349\n",
      "5 18 11\n",
      "edgeworth-parents.txt\n",
      "Number of characters: 935158\n",
      "Number of words: 210663\n",
      "Number of sentences: 10230\n",
      "Number of vocab: 8447\n",
      "4 21 25\n",
      "melville-moby_dick.txt\n",
      "Number of characters: 1242990\n",
      "Number of words: 260819\n",
      "Number of sentences: 10059\n",
      "Number of vocab: 17231\n",
      "5 26 15\n",
      "milton-paradise.txt\n",
      "Number of characters: 468220\n",
      "Number of words: 96825\n",
      "Number of sentences: 1851\n",
      "Number of vocab: 9021\n",
      "5 52 11\n",
      "shakespeare-caesar.txt\n",
      "Number of characters: 112310\n",
      "Number of words: 25833\n",
      "Number of sentences: 2163\n",
      "Number of vocab: 3032\n",
      "4 12 9\n",
      "shakespeare-hamlet.txt\n",
      "Number of characters: 162881\n",
      "Number of words: 37360\n",
      "Number of sentences: 3106\n",
      "Number of vocab: 4716\n",
      "4 12 8\n",
      "shakespeare-macbeth.txt\n",
      "Number of characters: 100351\n",
      "Number of words: 23140\n",
      "Number of sentences: 1907\n",
      "Number of vocab: 3464\n",
      "4 12 7\n",
      "whitman-leaves.txt\n",
      "Number of characters: 711215\n",
      "Number of words: 154883\n",
      "Number of sentences: 4250\n",
      "Number of vocab: 12452\n",
      "5 36 12\n"
     ]
    }
   ],
   "source": [
    "for fileid in gutenberg.fileids():\n",
    "    print(fileid)\n",
    "    num_chars = len(gutenberg.raw(fileid)) \n",
    "    num_words = len(gutenberg.words(fileid))\n",
    "    num_sents = len(gutenberg.sents(fileid))\n",
    "    num_vocab = len(set(w.lower() for w in gutenberg.words(fileid)))   # number of unique words\n",
    "#     print('Number of characters:', num_chars)\n",
    "#     print('Number of words:', num_words)\n",
    "#     print('Number of sentences:', num_sents)\n",
    "#     print('Number of vocab:', num_vocab)\n",
    "#     print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9534d7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'dost', 'thou', 'with', 'thy', 'best', 'Apparrell', 'on', '?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_sentences = gutenberg.sents('shakespeare-caesar.txt')\n",
    "macbeth_sentences[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1b796",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d44f1312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower(text):\n",
    "    # return the reviews after convering then to lowercase\n",
    "    # Words with different cases are intercepted differently such as 'The' and 'the'. \n",
    "    # Hence all words should be converted into same case, preferably lower case.\n",
    "    l = []\n",
    "    for t in text:\n",
    "        l.append(t.lower())\n",
    "    return l\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # return the reviews after removing punctuations\n",
    "    # Refer: https://www3.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html\n",
    "    l = []\n",
    "    # \\w : word character\n",
    "    # \\W : non-word character\n",
    "    # \\d : digits\n",
    "    # \\D : non-digits\n",
    "    \n",
    "    for t in text:\n",
    "        l.append(re.sub(r'[^\\w\\s]|^\\s\\d+\\s|\\s\\d+|\\d+|\\s\\d+$', ' ', t)) #|^\\s\\d+\\s|\\s\\d+|\\d+|\\s\\d+$\n",
    "    return l\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "#     return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # return the reviews after removing the stopwords\n",
    "    # Stopwords are the most common words in a language. For example 'is', 'the', 'that' etc. are stopwords in English language. Stopwords shall be removed during text clean-up phase. However removing stop word can change the meaning of sentence. \n",
    "    # For instance 'I didn't love politics' will get converted to 'I love politics' after removing stopword.  \n",
    "    l = []\n",
    "    large = 0\n",
    "    for t in text:\n",
    "        word_tokens = token.word_tokenize(t)\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "        l.append(filtered_sentence)\n",
    "    return l\n",
    "\n",
    "def remove_URLs(text):\n",
    "    text  = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text )\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    text= re.sub(r'[0-9]','',text)\n",
    "    return text\n",
    "\n",
    "def remove_spaces(text):\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def perform_tokenization(text):\n",
    "    # return the reviews after performing tokenization\n",
    "    text = token.word_tokenize(text)\n",
    "#     tk = Whitespa/ceTokenizer()\n",
    "#     tk = WordPunctTokenizer()\n",
    "#     tk = TreebankWordTokenizer()\n",
    "#     text = tk.tokenize(text)\n",
    "    \n",
    "    \n",
    "    return text\n",
    "\n",
    "def perform_padding(data):\n",
    "    # return the reviews after padding the reviews to maximum length\n",
    "    maxlen = 30\n",
    "    return pad_sequences(data, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "def correct_spellings(text):\n",
    "    # At times textual data such as social media data is prone to spelling errors. Spelling errors \n",
    "    # should be rectified early during the clean-up phase. Fortunately we have libraries available for spelling correction.\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_words.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "def convert_emoji(text):\n",
    "    text = emoji.demojize(text)\n",
    "    return text\n",
    "\n",
    "# def convert_to_antonym(sentence):\n",
    "#     words = nltk.word_tokenize(sentence)\n",
    "#     new_words = []\n",
    "#     temp_word = ''\n",
    "#     for word in words:\n",
    "#         antonyms = []\n",
    "#         if word == 'not':\n",
    "#             temp_word = 'not_'\n",
    "#         elif temp_word == 'not_':\n",
    "#             for syn in wordnet.synsets(word):\n",
    "#                 for s in syn.lemmas():\n",
    "#                     for a in s.antonyms():\n",
    "#                         antonyms.append(a.name())\n",
    "#             if len(antonyms) >= 1:\n",
    "#                 word = antonyms[0]\n",
    "#             else:\n",
    "#                 word = temp_word + word # when antonym is not found, it will\n",
    "#                                     # remain not_happy\n",
    "            \n",
    "#             temp_word = ''\n",
    "#         if word != 'not':\n",
    "#             new_words.append(word)\n",
    "#     return ' '.join(new_words)\n",
    "\n",
    "# stemmer = PorterStemmer()\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# # without wordnet map it takes evey word as noun\n",
    "# wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV }\n",
    "\n",
    "# def stem_words(text):\n",
    "#     # a process of removing and replacing suffixes to get the root form of the word.\n",
    "#     # Porterstemmer is rule based. (eg: dogs -> dog)\n",
    "#     return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "# def lemma_words(text):\n",
    "#     pos_tagged_text = nltk.pos_tag(text.split())\n",
    "#     return \" \".join([lemmatizer.lemmatize(word ,wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc7dfb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Where', 'is', 'thy', 'Leather', 'Apron', ' ', 'and', 'thy', 'Rule', ' ']\n"
     ]
    }
   ],
   "source": [
    "clean_sentences = remove_punctuation(macbeth_sentences[11])\n",
    "print(clean_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33d9419d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9e3eb6b41d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# return the reviews after convering then to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Words with different cases are intercepted differently such as 'The' and 'the'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Hence all words should be converted into same case, preferably lower case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "class Preprocessing:\n",
    "    def convert_to_lower(text):\n",
    "        # return the reviews after convering then to lowercase\n",
    "    # Words with different cases are intercepted differently such as 'The' and 'the'. \n",
    "    # Hence all words should be converted into same case, preferably lower case.\n",
    "        l = []\n",
    "        for t in text:\n",
    "            l.append(t.lower())\n",
    "        return l\n",
    "    \n",
    "    def remove_punctuation(text):\n",
    "        # return the reviews after removing punctuations\n",
    "        # Refer: https://www3.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html\n",
    "        l = []\n",
    "        # \\w : word character\n",
    "        # \\W : non-word character\n",
    "        # \\d : digits\n",
    "        # \\D : non-digits\n",
    "        for t in text:\n",
    "            l.append(re.sub(r'[^\\w\\s]|^\\s\\d+\\s|\\s\\d+|\\d+|\\s\\d+$', ' ', t)) #|^\\s\\d+\\s|\\s\\d+|\\d+|\\s\\d+$\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e2495a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Preprocessing() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-74019bd5fc35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmacbeth_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Preprocessing() takes no arguments"
     ]
    }
   ],
   "source": [
    "cleaned = Preprocessing(macbeth_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
